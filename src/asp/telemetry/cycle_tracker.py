"""
Self-Improvement Cycle Time Tracker

This module tracks the performance of the self-improvement loop:
- How long does it take from defect discovery to prompt improvement?
- What is the impact of prompt changes on defect rates?
- Are we getting better over time?

The tracker measures:
1. PIP Creation Time: Postmortem analysis → PIP generation
2. Review Cycle Time: PIP creation → human approval
3. Application Time: PIP approval → prompt update deployed
4. Impact Time: Prompt update → next task execution
5. Total Cycle Time: Defect discovered → improvement applied → validated

This enables data-driven insights into the improvement process itself.

Author: ASP Development Team
Date: November 25, 2025
"""

import logging
from datetime import datetime, timedelta
from pathlib import Path

from pydantic import BaseModel, Field

from asp.models.postmortem import ProcessImprovementProposal
from asp.utils.artifact_io import read_artifact_json, write_artifact_json

logger = logging.getLogger(__name__)


class CycleEvent(BaseModel):
    """
    Single event in the improvement cycle.

    Examples:
    - pip_created: PIP generated by Postmortem Agent
    - pip_reviewed: Human approved/rejected PIP
    - prompts_updated: Changes applied to prompts
    - next_task_started: First task using updated prompts
    - next_task_completed: First task completed with new prompts
    """

    event_type: str = Field(
        ...,
        description="Type of event (pip_created, pip_reviewed, prompts_updated, etc.)",
    )

    timestamp: datetime = Field(..., description="When this event occurred")

    metadata: dict[str, str] = Field(
        default_factory=dict, description="Additional event-specific metadata"
    )


class ImprovementCycle(BaseModel):
    """
    Complete improvement cycle tracking one PIP from creation to impact.

    Tracks the full lifecycle:
    1. Defects found in task X
    2. Postmortem analysis generates PIP
    3. Human reviews and approves PIP
    4. Prompt changes applied
    5. Task Y executes with updated prompts
    6. Compare defect rates between X and Y
    """

    pip_id: str = Field(..., description="PIP identifier (e.g., PIP-20251125230448)")

    task_id: str = Field(..., description="Task that triggered this PIP")

    events: list[CycleEvent] = Field(
        default_factory=list, description="Chronological list of cycle events"
    )

    target_artifacts: list[str] = Field(
        default_factory=list, description="Prompts/checklists modified by this PIP"
    )

    defect_types_targeted: list[str] = Field(
        default_factory=list, description="Defect types this PIP aims to prevent"
    )

    baseline_defect_count: int | None = Field(
        None, description="Number of target defects in triggering task"
    )

    post_improvement_defect_count: int | None = Field(
        None, description="Number of target defects in first task after improvement"
    )

    impact_task_id: str | None = Field(
        None,
        description="First task executed with updated prompts (for measuring impact)",
    )

    @property
    def review_cycle_time(self) -> timedelta | None:
        """Calculate time from PIP creation to approval."""
        created_event = next(
            (e for e in self.events if e.event_type == "pip_created"), None
        )
        reviewed_event = next(
            (e for e in self.events if e.event_type == "pip_reviewed"), None
        )

        if created_event and reviewed_event:
            return reviewed_event.timestamp - created_event.timestamp
        return None

    @property
    def total_cycle_time(self) -> timedelta | None:
        """Calculate total time from PIP creation to impact measurement."""
        created_event = next(
            (e for e in self.events if e.event_type == "pip_created"), None
        )
        impact_event = next(
            (e for e in self.events if e.event_type == "impact_measured"), None
        )

        if created_event and impact_event:
            return impact_event.timestamp - created_event.timestamp
        return None

    @property
    def defect_reduction_percent(self) -> float | None:
        """Calculate percentage reduction in target defects."""
        if (
            self.baseline_defect_count is not None
            and self.post_improvement_defect_count is not None
            and self.baseline_defect_count > 0
        ):
            reduction = (
                (self.baseline_defect_count - self.post_improvement_defect_count)
                / self.baseline_defect_count
            ) * 100
            return round(reduction, 1)
        return None


class CycleTracker:
    """
    Tracks and analyzes self-improvement cycles.

    The tracker:
    - Records events throughout PIP lifecycle
    - Calculates cycle time metrics
    - Measures improvement impact (defect reduction)
    - Generates reports on improvement velocity

    Example:
        >>> from asp.telemetry.cycle_tracker import CycleTracker
        >>>
        >>> tracker = CycleTracker()
        >>>
        >>> # When PIP is created
        >>> tracker.record_pip_created(pip)
        >>>
        >>> # When PIP is reviewed
        >>> tracker.record_pip_reviewed(pip)
        >>>
        >>> # When prompts are updated
        >>> tracker.record_prompts_updated(pip_id, prompt_files)
        >>>
        >>> # When next task completes
        >>> tracker.record_impact(pip_id, new_task_id, defect_count)
        >>>
        >>> # Analyze performance
        >>> cycle = tracker.get_cycle(pip.proposal_id)
        >>> print(f"Review time: {cycle.review_cycle_time}")
        >>> print(f"Defect reduction: {cycle.defect_reduction_percent}%")
    """

    def __init__(self, cycles_dir: Path = Path("artifacts/improvement_cycles")):
        """
        Initialize CycleTracker.

        Args:
            cycles_dir: Directory to store cycle tracking data
        """
        self.cycles_dir = cycles_dir
        self.cycles_dir.mkdir(parents=True, exist_ok=True)
        logger.info(f"CycleTracker initialized: {self.cycles_dir}")

    def record_pip_created(
        self,
        pip: ProcessImprovementProposal,
        defect_count: int | None = None,
    ) -> ImprovementCycle:
        """
        Record PIP creation event.

        Args:
            pip: Newly created ProcessImprovementProposal
            defect_count: Number of defects in triggering task

        Returns:
            ImprovementCycle object
        """
        logger.info(f"Recording PIP creation: {pip.proposal_id}")

        # Extract defect types from proposed changes
        defect_types = list(
            {change.target_artifact for change in pip.proposed_changes}
        )

        # Create cycle object
        cycle = ImprovementCycle(
            pip_id=pip.proposal_id,
            task_id=pip.task_id,
            events=[
                CycleEvent(
                    event_type="pip_created",
                    timestamp=pip.created_at,
                    metadata={
                        "pip_id": pip.proposal_id,
                        "task_id": pip.task_id,
                        "changes_count": str(len(pip.proposed_changes)),
                    },
                )
            ],
            target_artifacts=[
                change.target_artifact for change in pip.proposed_changes
            ],
            defect_types_targeted=defect_types,
            baseline_defect_count=defect_count,
        )

        # Save cycle
        self._save_cycle(cycle)

        return cycle

    def record_pip_reviewed(
        self,
        pip: ProcessImprovementProposal,
    ) -> ImprovementCycle:
        """
        Record PIP review event.

        Args:
            pip: Reviewed ProcessImprovementProposal

        Returns:
            Updated ImprovementCycle object
        """
        logger.info(f"Recording PIP review: {pip.proposal_id} -> {pip.hitl_status}")

        # Load existing cycle
        cycle = self._load_cycle(pip.proposal_id)

        # Add review event
        cycle.events.append(
            CycleEvent(
                event_type="pip_reviewed",
                timestamp=pip.hitl_reviewed_at or datetime.now(),
                metadata={
                    "decision": pip.hitl_status,
                    "reviewer": pip.hitl_reviewer or "unknown",
                    "feedback": pip.hitl_feedback or "",
                },
            )
        )

        # Save updated cycle
        self._save_cycle(cycle)

        return cycle

    def record_prompts_updated(
        self,
        pip_id: str,
        updated_files: list[str],
    ) -> ImprovementCycle:
        """
        Record prompt update event.

        Args:
            pip_id: PIP identifier
            updated_files: List of prompt files that were updated

        Returns:
            Updated ImprovementCycle object
        """
        logger.info(f"Recording prompt updates for PIP: {pip_id}")

        # Load existing cycle
        cycle = self._load_cycle(pip_id)

        # Add prompts_updated event
        cycle.events.append(
            CycleEvent(
                event_type="prompts_updated",
                timestamp=datetime.now(),
                metadata={
                    "files_updated": ", ".join(updated_files),
                    "count": str(len(updated_files)),
                },
            )
        )

        # Save updated cycle
        self._save_cycle(cycle)

        return cycle

    def record_impact(
        self,
        pip_id: str,
        impact_task_id: str,
        defect_count: int,
        notes: str | None = None,
    ) -> ImprovementCycle:
        """
        Record impact measurement from first task after improvement.

        Args:
            pip_id: PIP identifier
            impact_task_id: Task ID of first task with updated prompts
            defect_count: Number of target defects in impact task
            notes: Optional notes about impact

        Returns:
            Updated ImprovementCycle object
        """
        logger.info(
            f"Recording impact for PIP {pip_id}: "
            f"task={impact_task_id}, defects={defect_count}"
        )

        # Load existing cycle
        cycle = self._load_cycle(pip_id)

        # Update impact fields
        cycle.impact_task_id = impact_task_id
        cycle.post_improvement_defect_count = defect_count

        # Add impact_measured event
        metadata = {
            "impact_task_id": impact_task_id,
            "defect_count": str(defect_count),
            "baseline_defect_count": str(cycle.baseline_defect_count or 0),
        }

        if notes:
            metadata["notes"] = notes

        if cycle.defect_reduction_percent is not None:
            metadata["defect_reduction_percent"] = f"{cycle.defect_reduction_percent}%"

        cycle.events.append(
            CycleEvent(
                event_type="impact_measured",
                timestamp=datetime.now(),
                metadata=metadata,
            )
        )

        # Save updated cycle
        self._save_cycle(cycle)

        logger.info(
            f"Impact recorded: {cycle.baseline_defect_count} → {defect_count} defects "
            f"({cycle.defect_reduction_percent}% reduction)"
        )

        return cycle

    def get_cycle(self, pip_id: str) -> ImprovementCycle:
        """
        Get improvement cycle for a PIP.

        Args:
            pip_id: PIP identifier

        Returns:
            ImprovementCycle object

        Raises:
            FileNotFoundError: If cycle not found
        """
        return self._load_cycle(pip_id)

    def get_all_cycles(self) -> list[ImprovementCycle]:
        """
        Get all improvement cycles.

        Returns:
            List of ImprovementCycle objects
        """
        cycles = []

        for cycle_file in self.cycles_dir.glob("*.json"):
            try:
                cycle_data = read_artifact_json(
                    task_id=cycle_file.stem,
                    artifact_type="improvement_cycle",
                    artifacts_dir=self.cycles_dir.parent,
                )
                cycle = ImprovementCycle(**cycle_data)
                cycles.append(cycle)
            except Exception as e:
                logger.warning(f"Failed to load cycle from {cycle_file}: {e}")

        return cycles

    def generate_report(self) -> dict[str, any]:
        """
        Generate summary report of all improvement cycles.

        Returns:
            Dict with cycle time metrics and impact analysis
        """
        cycles = self.get_all_cycles()

        if not cycles:
            return {"total_cycles": 0, "message": "No improvement cycles tracked yet"}

        # Calculate aggregate metrics
        review_times = [
            c.review_cycle_time.total_seconds() / 3600  # hours
            for c in cycles
            if c.review_cycle_time is not None
        ]

        total_times = [
            c.total_cycle_time.total_seconds() / 3600  # hours
            for c in cycles
            if c.total_cycle_time is not None
        ]

        defect_reductions = [
            c.defect_reduction_percent
            for c in cycles
            if c.defect_reduction_percent is not None
        ]

        report = {
            "total_cycles": len(cycles),
            "completed_cycles": len(total_times),
            "avg_review_time_hours": (
                round(sum(review_times) / len(review_times), 2)
                if review_times
                else None
            ),
            "avg_total_cycle_time_hours": (
                round(sum(total_times) / len(total_times), 2) if total_times else None
            ),
            "avg_defect_reduction_percent": (
                round(sum(defect_reductions) / len(defect_reductions), 1)
                if defect_reductions
                else None
            ),
            "cycles_with_positive_impact": sum(1 for r in defect_reductions if r > 0),
            "cycles_with_negative_impact": sum(1 for r in defect_reductions if r < 0),
            "cycles_with_no_change": sum(1 for r in defect_reductions if r == 0),
        }

        return report

    def _load_cycle(self, pip_id: str) -> ImprovementCycle:
        """
        Load improvement cycle from disk.

        Args:
            pip_id: PIP identifier

        Returns:
            ImprovementCycle object

        Raises:
            FileNotFoundError: If cycle file not found
        """
        cycle_file = self.cycles_dir / f"{pip_id}.json"

        if not cycle_file.exists():
            raise FileNotFoundError(f"Improvement cycle not found for PIP: {pip_id}")

        cycle_data = read_artifact_json(
            task_id=pip_id,
            artifact_type="improvement_cycle",
            artifacts_dir=self.cycles_dir.parent,
        )

        return ImprovementCycle(**cycle_data)

    def _save_cycle(self, cycle: ImprovementCycle) -> None:
        """
        Save improvement cycle to disk.

        Args:
            cycle: ImprovementCycle to save
        """
        write_artifact_json(
            task_id=cycle.pip_id,
            artifact_type="improvement_cycle",
            data=cycle,
            artifacts_dir=self.cycles_dir.parent,
        )

        logger.debug(f"Saved improvement cycle: {cycle.pip_id}")
