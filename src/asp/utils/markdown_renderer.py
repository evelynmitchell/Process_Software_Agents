"""
Markdown rendering utilities for ASP platform.

This module provides functions for converting agent artifacts
(Pydantic models) into human-readable Markdown format.

Implements the markdown rendering from:
docs/artifact_persistence_version_control_decision.md

Author: ASP Development Team
Date: November 17, 2025
"""

import logging
from datetime import datetime
from typing import Any

from asp.models.code import GeneratedCode
from asp.models.code_review import CodeReviewReport
from asp.models.design import DesignSpecification
from asp.models.design_review import DesignReviewReport
from asp.models.planning import ProjectPlan
from asp.models.test import TestReport

logger = logging.getLogger(__name__)


def render_plan_markdown(plan: ProjectPlan) -> str:
    """
    Render ProjectPlan as human-readable Markdown.

    Args:
        plan: ProjectPlan Pydantic model

    Returns:
        Markdown formatted string

    Example:
        >>> markdown = render_plan_markdown(project_plan)
        >>> "# Project Plan:" in markdown
        True
    """
    # Build header with available fields
    md = f"""# Project Plan: {plan.task_id}

**Project ID:** {plan.project_id or "N/A"}
**Task ID:** {plan.task_id}
**Total Complexity:** {plan.total_est_complexity}
**PROBE-AI Enabled:** {plan.probe_ai_enabled}
**Agent Version:** {plan.agent_version}

"""

    # Add PROBE-AI predictions if available
    if plan.probe_ai_prediction:
        md += f"""## PROBE-AI Predictions

**Estimated Effort:** {plan.probe_ai_prediction.total_effort_hours:.1f} hours
**Estimated Cost:** ${plan.probe_ai_prediction.total_cost:.2f}
**Confidence:** {plan.probe_ai_prediction.confidence:.1%}

"""

    md += "## Task Decomposition\n\n"

    # Add semantic units
    for i, unit in enumerate(plan.semantic_units, 1):
        md += f"""### {unit.unit_id}: {unit.description}

- **Estimated Complexity:** {unit.est_complexity}
- **API Interactions:** {unit.api_interactions}
- **Data Transformations:** {unit.data_transformations}
- **Logical Branches:** {unit.logical_branches}
- **Code Entities Modified:** {unit.code_entities_modified}
- **Novelty Multiplier:** {unit.novelty_multiplier}
- **Dependencies:** {", ".join(unit.dependencies) if unit.dependencies else "None"}

"""

    return md


def render_design_markdown(design: DesignSpecification) -> str:
    """
    Render DesignSpecification as human-readable Markdown.

    Args:
        design: DesignSpecification Pydantic model

    Returns:
        Markdown formatted string
    """
    md = f"""# Design Specification: {design.task_id}

**Task ID:** {design.task_id}

## Architecture Overview

{design.architecture_overview}

## Technology Stack

{design.technology_stack}

## Assumptions

{design.assumptions}

"""

    # Add API contracts
    if design.api_contracts:
        md += "## API Contracts\n\n"
        for api in design.api_contracts:
            md += f"""### {api.method} {api.endpoint}

- **Description:** {api.description}
- **Authentication:** {api.authentication_required}
"""
            if api.request_schema:
                md += f"- **Request Schema:**\n```json\n{api.request_schema}\n```\n"
            if api.response_schema:
                md += f"- **Response Schema:**\n```json\n{api.response_schema}\n```\n"
            if api.error_responses:
                # error_responses is a list of dicts with status_code and description
                error_summary = ", ".join([f"{e.get('status_code', 'N/A')}" for e in api.error_responses])
                md += f"- **Error Responses:** {error_summary}\n"

            md += "\n"

    # Add data schemas
    if design.data_schemas:
        md += "## Data Schemas\n\n"
        for schema in design.data_schemas:
            md += f"""### {schema.table_name}

- **Description:** {schema.description}
"""
            if schema.fields:
                md += "- **Fields:**\n"
                for field in schema.fields:
                    md += f"  - `{field}`\n"
            if schema.indexes:
                md += f"- **Indexes:** {', '.join(schema.indexes)}\n"
            if schema.relationships:
                md += f"- **Relationships:** {', '.join(schema.relationships)}\n"

            md += "\n"

    # Add component logic
    if design.component_logic:
        md += "## Component Logic\n\n"
        for component in design.component_logic:
            md += f"""### {component.component_name}

- **Responsibility:** {component.responsibility}
- **Semantic Unit:** {component.semantic_unit_id}
- **Dependencies:** {', '.join(component.dependencies) if component.dependencies else 'None'}
- **Implementation Notes:** {component.implementation_notes}
"""
            if component.interfaces:
                md += "- **Interfaces:**\n"
                for interface in component.interfaces:
                    md += f"  - `{interface.get('method', 'N/A')}`\n"

            md += "\n"

    # Add metadata
    md += f"""---

*Generated by Design Agent on {design.timestamp.strftime('%Y-%m-%d %H:%M:%S')}*
"""

    return md


def render_design_review_markdown(review: DesignReviewReport) -> str:
    """
    Render DesignReviewReport as human-readable Markdown.

    Args:
        review: DesignReviewReport Pydantic model

    Returns:
        Markdown formatted string
    """
    md = f"""# Design Review Report: {review.task_id}

**Review ID:** {review.review_id}
**Review Status:** {review.overall_assessment}
**Reviewed by:** Design Review Agent v{review.agent_version}
**Date:** {review.timestamp.strftime('%Y-%m-%d %H:%M:%S')}

## Summary

- **Total Issues:** {review.total_issues}
- **Critical:** {review.critical_issues}
- **High:** {review.high_issues}
- **Medium:** {review.medium_issues}
- **Low:** {review.low_issues}

"""

    # Add issues by severity
    if review.critical_issues > 0:
        md += "## Critical Issues\n\n"
        for issue in review.issues_found:
            if issue.severity == "Critical":
                md += f"""### {issue.issue_id}: {issue.description}

**Category:** {issue.category}
**Severity:** {issue.severity}
**Affected Phase:** {issue.affected_phase}
**Evidence:** {issue.evidence}

{issue.impact}

"""

    if review.high_issues > 0:
        md += "## High Issues\n\n"
        for issue in review.issues_found:
            if issue.severity == "High":
                md += f"""### {issue.issue_id}: {issue.description}

**Category:** {issue.category}
**Evidence:** {issue.evidence}

{issue.impact}

"""

    if review.medium_issues > 0:
        md += "## Medium Issues\n\n"
        for issue in review.issues_found:
            if issue.severity == "Medium":
                md += f"- **{issue.issue_id}**: {issue.description} ({issue.category})\n"
        md += "\n"

    if review.low_issues > 0:
        md += "## Low Issues\n\n"
        for issue in review.issues_found:
            if issue.severity == "Low":
                md += f"- **{issue.issue_id}**: {issue.description} ({issue.category})\n"
        md += "\n"

    # Add improvement suggestions
    if review.improvement_suggestions:
        md += "## Improvement Suggestions\n\n"
        for suggestion in review.improvement_suggestions:
            if suggestion.priority == "High":
                md += f"""### {suggestion.suggestion_id}: {suggestion.description}

**Priority:** {suggestion.priority}
**Category:** {suggestion.category}
"""
                if suggestion.related_issue_id:
                    md += f"**Addresses:** {suggestion.related_issue_id}\n"

                md += f"\n{suggestion.implementation_notes}\n\n"

    # Add phase-aware breakdown
    if review.planning_phase_issues or review.design_phase_issues:
        md += "## Phase-Aware Issue Breakdown\n\n"
        if review.planning_phase_issues:
            md += f"- **Planning Phase Issues:** {len(review.planning_phase_issues)}\n"
        if review.design_phase_issues:
            md += f"- **Design Phase Issues:** {len(review.design_phase_issues)}\n"
        md += "\n"

    # Add metadata
    if review.review_duration_seconds:
        md += f"""---

*Review completed in {review.review_duration_seconds:.1f} seconds*
"""

    return md


def render_code_manifest_markdown(code: GeneratedCode) -> str:
    """
    Render GeneratedCode metadata as human-readable Markdown.

    Args:
        code: GeneratedCode Pydantic model

    Returns:
        Markdown formatted string
    """
    md = f"""# Code Generation Manifest: {code.task_id}

**Project ID:** {code.project_id or "N/A"}
**Generated by:** Code Agent v{code.agent_version}
**Timestamp:** {code.generation_timestamp or "N/A"}

## Summary

- **Total Files:** {code.total_files}
- **Total Lines of Code:** {code.total_lines_of_code:,}
- **Test Coverage Target:** {code.test_coverage_target or 0}%

## File Structure

"""

    # Add file structure
    for directory, files in sorted(code.file_structure.items()):
        md += f"**{directory}/**\n"
        for file in sorted(files):
            md += f"- {file}\n"
        md += "\n"

    # Add generated files summary
    md += "## Generated Files\n\n"

    # Group by file type
    source_files = [f for f in code.files if f.file_type == "source"]
    test_files = [f for f in code.files if f.file_type == "test"]
    config_files = [f for f in code.files if f.file_type == "config"]
    other_files = [f for f in code.files if f.file_type not in ["source", "test", "config"]]

    if source_files:
        md += "### Source Files\n\n"
        for file in source_files:
            md += f"- `{file.file_path}` - {file.description}\n"
        md += "\n"

    if test_files:
        md += "### Test Files\n\n"
        for file in test_files:
            md += f"- `{file.file_path}` - {file.description}\n"
        md += "\n"

    if config_files:
        md += "### Configuration Files\n\n"
        for file in config_files:
            md += f"- `{file.file_path}` - {file.description}\n"
        md += "\n"

    # Add implementation notes
    md += f"""## Implementation Notes

{code.implementation_notes}

"""

    # Add dependencies
    if code.dependencies:
        md += "## Dependencies\n\n"
        md += "```\n"
        for dep in code.dependencies:
            md += f"{dep}\n"
        md += "```\n\n"

    # Add setup instructions
    if code.setup_instructions:
        md += f"""## Setup Instructions

{code.setup_instructions}

"""

    # Add traceability
    if code.semantic_units_implemented or code.components_implemented:
        md += "## Traceability\n\n"
        if code.semantic_units_implemented:
            md += f"- **Semantic Units:** {', '.join(code.semantic_units_implemented)}\n"
        if code.components_implemented:
            md += f"- **Components:** {', '.join(code.components_implemented)}\n"
        md += "\n"

    md += "---\n\n*See individual files in src/, tests/, etc. for actual code*\n"

    return md


def render_code_review_markdown(review: CodeReviewReport) -> str:
    """
    Render CodeReviewReport as human-readable Markdown.

    Args:
        review: CodeReviewReport Pydantic model

    Returns:
        Markdown formatted string
    """
    md = f"""# Code Review Report: {review.task_id}

**Review ID:** {review.review_id}
**Review Status:** {review.overall_assessment}
**Reviewed by:** Code Review Agent v{review.agent_version}
**Date:** {review.timestamp.strftime('%Y-%m-%d %H:%M:%S')}

## Summary

- **Files Reviewed:** {review.files_reviewed}
- **Lines Reviewed:** {review.total_lines_reviewed:,}
- **Total Issues:** {review.total_issues}
- **Critical:** {review.critical_issues}
- **High:** {review.high_issues}
- **Medium:** {review.medium_issues}
- **Low:** {review.low_issues}

"""

    # Add critical issues
    if review.critical_issues > 0:
        md += "## Critical Issues\n\n"
        for issue in review.issues_found:
            if issue.severity == "Critical":
                md += f"""### {issue.issue_id}: {issue.description}

**Category:** {issue.category}
**Severity:** {issue.severity}
**Affected Phase:** {issue.affected_phase}
**File:** `{issue.file_path}`
"""
                if issue.line_number:
                    md += f"**Line:** {issue.line_number}\n"

                if issue.code_snippet:
                    md += f"\n```python\n{issue.code_snippet}\n```\n"

                md += f"\n**Impact:** {issue.impact}\n\n"

    # Add high issues
    if review.high_issues > 0:
        md += "## High Issues\n\n"
        for issue in review.issues_found:
            if issue.severity == "High":
                md += f"""### {issue.issue_id}: {issue.description}

**Category:** {issue.category}
**File:** `{issue.file_path}:{issue.line_number or 'N/A'}`

{issue.impact}

"""

    # Add medium/low issues summary
    if review.medium_issues > 0 or review.low_issues > 0:
        md += "## Other Issues\n\n"
        for issue in review.issues_found:
            if issue.severity in ["Medium", "Low"]:
                md += f"- **[{issue.severity}]** {issue.issue_id}: {issue.description} (`{issue.file_path}`)\n"
        md += "\n"

    # Add improvement suggestions
    if review.improvement_suggestions:
        md += "## Improvement Suggestions\n\n"
        for suggestion in review.improvement_suggestions:
            if suggestion.priority == "High":
                md += f"""### {suggestion.suggestion_id}: {suggestion.description}

**Priority:** {suggestion.priority}
**Category:** {suggestion.category}
"""
                if suggestion.related_issue_id:
                    md += f"**Fixes:** {suggestion.related_issue_id}\n"

                md += f"\n{suggestion.implementation_notes}\n"

                if suggestion.suggested_code:
                    md += f"\n**Suggested Fix:**\n```python\n{suggestion.suggested_code}\n```\n"

                md += "\n"

    # Add specialist results
    md += """## Specialist Review Results

"""

    specialists = [
        ("Security Review", review.security_review_passed),
        ("Code Quality Review", review.quality_review_passed),
        ("Performance Review", review.performance_review_passed),
        ("Standards Compliance", review.standards_review_passed),
        ("Testing Review", review.testing_review_passed),
        ("Maintainability Review", review.maintainability_review_passed),
    ]

    for name, passed in specialists:
        status_text = "PASS" if passed else "FAIL"
        md += f"- {name}: {status_text}\n"

    md += "\n"

    # Add phase-aware breakdown
    if review.planning_phase_issues or review.design_phase_issues or review.code_phase_issues:
        md += "## Phase-Aware Issue Breakdown\n\n"
        if review.planning_phase_issues:
            md += f"- **Planning Phase Issues:** {len(review.planning_phase_issues)}\n"
        if review.design_phase_issues:
            md += f"- **Design Phase Issues:** {len(review.design_phase_issues)}\n"
        if review.code_phase_issues:
            md += f"- **Code Phase Issues:** {len(review.code_phase_issues)}\n"
        md += "\n"

    # Add metadata
    if review.review_duration_seconds:
        md += f"""---

*Review completed in {review.review_duration_seconds:.1f} seconds*
"""

    return md


def render_test_report_markdown(report: TestReport) -> str:
    """
    Render TestReport as human-readable Markdown.

    Args:
        report: TestReport Pydantic model

    Returns:
        Markdown formatted string
    """
    # Status emoji
    status_emoji = {
        "PASS": "âœ…",
        "FAIL": "âŒ",
        "BUILD_FAILED": "ðŸ”´",
    }
    emoji = status_emoji.get(report.test_status, "â“")

    md = f"""# Test Report: {report.task_id}

**Test Status:** {emoji} {report.test_status}
**Tested by:** Test Agent v{report.agent_version}
**Date:** {report.test_timestamp}
**Duration:** {report.test_duration_seconds:.1f}s

## Build Status

**Build Successful:** {"âœ… Yes" if report.build_successful else "âŒ No"}

"""

    if report.build_errors:
        md += "### Build Errors\n\n"
        for error in report.build_errors:
            md += f"- {error}\n"
        md += "\n"

    # Test execution summary
    md += f"""## Test Execution Summary

- **Total Tests:** {report.test_summary.get('total_tests', 0)}
- **Passed:** {report.test_summary.get('passed', 0)} âœ…
- **Failed:** {report.test_summary.get('failed', 0)} âŒ
- **Skipped:** {report.test_summary.get('skipped', 0)} â­ï¸
- **Coverage:** {report.coverage_percentage or 'N/A'}%

## Test Generation

- **Tests Generated:** {report.total_tests_generated}
- **Test Files Created:** {len(report.test_files_created)}

"""

    if report.test_files_created:
        for file in report.test_files_created:
            md += f"  - `{file}`\n"
        md += "\n"

    # Defects summary
    md += f"""## Defects Summary

- **Total Defects:** {len(report.defects_found)}
- **Critical:** {report.critical_defects} ðŸ”´
- **High:** {report.high_defects} ðŸŸ 
- **Medium:** {report.medium_defects} ðŸŸ¡
- **Low:** {report.low_defects} ðŸŸ¢

"""

    # Critical defects
    if report.critical_defects > 0:
        md += "## Critical Defects\n\n"
        for defect in report.defects_found:
            if defect.severity == "Critical":
                md += f"""### {defect.defect_id}: {defect.description}

**Type:** {defect.defect_type}
**Severity:** {defect.severity}
**Phase Injected:** {defect.phase_injected}
**File:** `{defect.file_path or 'N/A'}:{defect.line_number or 'N/A'}`

**Evidence:**
```
{defect.evidence}
```

**Impact:** This is a critical defect that must be fixed before proceeding.

---

"""

    # High defects
    if report.high_defects > 0:
        md += "## High Priority Defects\n\n"
        for defect in report.defects_found:
            if defect.severity == "High":
                md += f"""### {defect.defect_id}: {defect.description}

**Type:** {defect.defect_type}
**Phase Injected:** {defect.phase_injected}
**File:** `{defect.file_path or 'N/A'}:{defect.line_number or 'N/A'}`

**Evidence:**
```
{defect.evidence}
```

---

"""

    # Medium and low defects (summary only)
    if report.medium_defects > 0 or report.low_defects > 0:
        md += "## Other Defects\n\n"
        for defect in report.defects_found:
            if defect.severity in ["Medium", "Low"]:
                md += f"- **[{defect.severity}]** {defect.defect_id}: {defect.description} "
                md += f"({defect.defect_type}) - `{defect.file_path or 'N/A'}`\n"
        md += "\n"

    # Defect analysis
    if len(report.defects_found) > 0:
        md += "## Defect Analysis\n\n"

        # Group by phase injected
        phases = {}
        for defect in report.defects_found:
            phase = defect.phase_injected
            if phase not in phases:
                phases[phase] = []
            phases[phase].append(defect)

        md += "### Defects by Phase Injected\n\n"
        for phase, defects in sorted(phases.items()):
            md += f"- **{phase}:** {len(defects)} defects\n"
        md += "\n"

        # Group by defect type
        types = {}
        for defect in report.defects_found:
            dtype = defect.defect_type
            if dtype not in types:
                types[dtype] = []
            types[dtype].append(defect)

        md += "### Defects by Type\n\n"
        for dtype, defects in sorted(types.items()):
            md += f"- **{dtype}:** {len(defects)} defects\n"
        md += "\n"

    # Recommendations
    if report.test_status != "PASS":
        md += "## Recommendations\n\n"

        if report.test_status == "BUILD_FAILED":
            md += """### Immediate Actions Required

1. Fix all build errors before proceeding
2. Verify all dependencies are installed
3. Check import statements and module paths
4. Return to Code Agent for corrections

"""
        elif report.critical_defects > 0 or report.high_defects > 0:
            md += """### Immediate Actions Required

1. Address all Critical and High severity defects
2. Re-run tests after fixes
3. Return to Code Agent if needed

"""
        else:
            md += """### Suggested Actions

1. Review and address Medium/Low severity defects
2. Consider proceeding with caution
3. Document known issues for future work

"""

    # Footer
    md += f"""---

*Test report generated by Test Agent v{report.agent_version}*
"""

    return md
