# Telemetry User Guide

## Overview

The ASP Platform includes a comprehensive telemetry infrastructure that tracks agent execution metrics, costs, and defects. This guide explains how to access and interpret telemetry data through both Langfuse Cloud and the local SQLite database.

## Table of Contents

1. [Telemetry Architecture](#telemetry-architecture)
2. [Accessing Langfuse Dashboard](#accessing-langfuse-dashboard)
3. [Understanding Langfuse Data](#understanding-langfuse-data)
4. [Querying Local Database](#querying-local-database)
5. [Telemetry Data Schema](#telemetry-data-schema)
6. [Common Queries](#common-queries)
7. [Troubleshooting](#troubleshooting)

---

## Telemetry Architecture

The ASP Platform uses a dual-storage approach for telemetry:

1. **Langfuse Cloud** - Real-time observability and visualization
   - Web-based dashboard
   - Traces, spans, and events
   - Usage analytics and charts
   - Team collaboration features

2. **Local SQLite Database** - Persistent storage for analysis
   - Location: `data/asp_telemetry.db`
   - Structured tables for metrics and defects
   - Supports custom queries and analysis
   - Used for PROBE-AI model training

### What Gets Tracked

**Agent Execution Metrics:**
- Execution latency (milliseconds)
- Token usage (input/output tokens)
- API costs (USD)
- Success/failure status
- LLM model and provider

**Defect Tracking:**
- Defect type and severity
- Phase injected vs phase removed
- Component location (file, function, line)
- Root cause and resolution notes
- Time to fix

---

## Accessing Langfuse Dashboard

### Step 1: Navigate to Langfuse Cloud

**URL:** https://us.cloud.langfuse.com

### Step 2: Login

Use your Langfuse account credentials to login. If you don't have an account:
1. Go to https://cloud.langfuse.com
2. Click "Sign Up"
3. Create an account (free tier available)

### Step 3: Select Your Project

After logging in:
1. Select your ASP Platform project from the dashboard
2. If you haven't created a project yet:
   - Click "New Project"
   - Name it (e.g., "ASP Platform")
   - Copy the API keys to your secrets

### Step 4: Configure API Keys

Your project needs the following environment variables:

```bash
# Add to GitHub Codespaces secrets or .env file
LANGFUSE_PUBLIC_KEY=pk-lf-your-public-key-here
LANGFUSE_SECRET_KEY=sk-lf-your-secret-key-here
LANGFUSE_HOST=https://cloud.langfuse.com
```

**Getting your keys:**
1. In Langfuse dashboard, go to Project Settings
2. Navigate to API Keys section
3. Copy the Public Key and Secret Key
4. Add them to your GitHub Codespaces secrets or `.env` file

---

## Understanding Langfuse Data

### Traces

A **trace** represents a complete execution flow. Each agent execution creates a trace.

**Example trace name:** `Planning.execute`, `Design.execute`, `DesignReview.execute`

**Trace metadata includes:**
- `agent_role` - Agent type (Planning, Design, Code, etc.)
- `task_id` - Unique task identifier
- `function` - Function name being executed
- `llm_model` - LLM model used (e.g., "claude-sonnet-4")
- `llm_provider` - Provider name (e.g., "anthropic")
- `agent_version` - Agent version string

### Spans

**Spans** are sub-operations within a trace. For multi-agent systems like the Design Review Agent, you'll see:

```
DesignReview.execute                 (parent span)
├── SecurityReviewAgent.execute      (child span)
├── PerformanceReviewAgent.execute   (child span)
├── DataIntegrityReviewAgent.execute (child span)
├── MaintainabilityReviewAgent.execute (child span)
├── ArchitectureReviewAgent.execute  (child span)
└── APIDesignReviewAgent.execute     (child span)
```

### Usage Metrics

Each span includes usage metrics:
- **Input tokens** - Tokens sent to LLM
- **Output tokens** - Tokens generated by LLM
- **Total tokens** - Sum of input + output
- **Latency** - Execution time in milliseconds

### Events

**Events** are discrete occurrences logged during execution:

**Defect events:**
- Event name: `defect.{defect_type}`
- Example: `defect.6_Conventional_Code_Bug`
- Includes severity, phases, and fix time

### Navigation Tips

**Viewing Traces:**
1. Click "Traces" in the left sidebar
2. Filter by agent_role, task_id, or time range
3. Click a trace to see detailed breakdown

**Analyzing Usage:**
1. Click "Analytics" in the left sidebar
2. View token usage over time
3. Track costs per agent
4. Identify expensive operations

**Searching:**
1. Use the search bar at the top
2. Search by task_id: `task_id:TASK-2025-001`
3. Search by agent: `agent_role:Planning`
4. Search by model: `llm_model:claude-sonnet-4`

---

## Querying Local Database

### Database Location

```
data/asp_telemetry.db
```

### Connection Methods

#### Method 1: Python Script

```python
import sqlite3

# Connect to database
conn = sqlite3.connect('data/asp_telemetry.db')
conn.row_factory = sqlite3.Row  # Enable column access by name

# Execute query
cursor = conn.cursor()
cursor.execute('SELECT * FROM agent_cost_vector LIMIT 10')

# Fetch results
for row in cursor.fetchall():
    print(dict(row))

conn.close()
```

#### Method 2: Command Line (uv run)

```bash
# Simple query
uv run python -c "
import sqlite3
conn = sqlite3.connect('data/asp_telemetry.db')
cursor = conn.cursor()
cursor.execute('SELECT task_id, agent_role, metric_value FROM agent_cost_vector LIMIT 10')
for row in cursor.fetchall():
    print(row)
conn.close()
"
```

#### Method 3: SQLite CLI

```bash
# Install sqlite3 if needed
sudo apt-get install sqlite3

# Open database
sqlite3 data/asp_telemetry.db

# Run queries interactively
.tables
.schema agent_cost_vector
SELECT * FROM agent_cost_vector LIMIT 10;
```

#### Method 4: Database Browser

Use a GUI tool like:
- [DB Browser for SQLite](https://sqlitebrowser.org/) (free)
- [SQLite Studio](https://sqlitestudio.pl/) (free)
- VS Code SQLite extension

---

## Telemetry Data Schema

### Table: `agent_cost_vector`

Stores all agent execution metrics.

| Column | Type | Description |
|--------|------|-------------|
| id | INTEGER | Auto-increment primary key |
| timestamp | TEXT | ISO 8601 timestamp (UTC) |
| task_id | TEXT | Task identifier (required) |
| subtask_id | TEXT | Subtask identifier (optional) |
| project_id | TEXT | Project identifier (optional) |
| agent_role | TEXT | Agent role (Planning, Design, Code, etc.) |
| agent_version | TEXT | Agent version string |
| agent_iteration | INTEGER | Iteration number for retries |
| metric_type | TEXT | Latency, Tokens_In, Tokens_Out, API_Cost, etc. |
| metric_value | REAL | Numeric value of the metric |
| metric_unit | TEXT | Unit (ms, tokens, USD, MB, count) |
| llm_model | TEXT | LLM model name |
| llm_provider | TEXT | LLM provider (anthropic, openai, etc.) |
| metadata | TEXT | JSON metadata |

**Metric Types:**
- `Latency` - Execution time (ms)
- `Tokens_In` - Input tokens sent to LLM
- `Tokens_Out` - Output tokens generated by LLM
- `API_Cost` - API cost in USD
- `Memory_Peak` - Peak memory usage (MB)
- `Cache_Hit` - Cache hit count
- `Custom` - Custom metrics

**Indexes:**
- `idx_agent_cost_task_id` on `task_id`
- `idx_agent_cost_agent_role` on `agent_role`
- `idx_agent_cost_timestamp` on `timestamp`

### Table: `defect_log`

Stores defect tracking information.

| Column | Type | Description |
|--------|------|-------------|
| id | INTEGER | Auto-increment primary key |
| defect_id | TEXT | Unique defect ID (DEFECT-XXXX) |
| created_at | TEXT | ISO 8601 timestamp (UTC) |
| task_id | TEXT | Task identifier (required) |
| project_id | TEXT | Project identifier (optional) |
| defect_type | TEXT | Defect type (see types below) |
| severity | TEXT | Low, Medium, High, Critical |
| description | TEXT | Defect description (required) |
| phase_injected | TEXT | Phase where defect was introduced |
| phase_removed | TEXT | Phase where defect was detected/fixed |
| component_path | TEXT | File path of affected component |
| function_name | TEXT | Function name where defect exists |
| line_number | INTEGER | Line number of defect |
| root_cause | TEXT | Root cause analysis |
| resolution_notes | TEXT | How the defect was resolved |
| flagged_by_agent | INTEGER | 1 if flagged by agent, 0 if human |
| metadata | TEXT | JSON metadata |

**Defect Types:**
1. `1_Specification_Requirements` - Spec/requirement issues
2. `2_Design_System_Architecture` - Design/architecture defects
3. `3_UI_UX_Usability` - UI/UX issues
4. `4_Data_Model_Schema` - Data model defects
5. `5_Business_Logic` - Business logic errors
6. `6_Conventional_Code_Bug` - Standard code bugs
7. `7_Security_Vulnerability` - Security issues
8. `8_Performance_Efficiency` - Performance problems
9. `9_Integration_API` - Integration/API issues
10. `10_Testing_Quality_Assurance` - Testing defects

**Severity Levels:**
- `Low` - Minor issues, cosmetic problems
- `Medium` - Moderate impact, workarounds available
- `High` - Significant impact, needs urgent attention
- `Critical` - System-breaking, immediate action required

**Indexes:**
- `idx_defect_log_task_id` on `task_id`
- `idx_defect_log_defect_id` on `defect_id`
- `idx_defect_log_severity` on `severity`

---

## Common Queries

### Agent Performance Analysis

#### 1. Total Cost by Agent Role

```sql
SELECT
    agent_role,
    COUNT(*) as execution_count,
    SUM(CASE WHEN metric_type = 'API_Cost' THEN metric_value ELSE 0 END) as total_cost,
    AVG(CASE WHEN metric_type = 'Latency' THEN metric_value ELSE 0 END) as avg_latency_ms
FROM agent_cost_vector
GROUP BY agent_role
ORDER BY total_cost DESC;
```

#### 2. Token Usage by Task

```sql
SELECT
    task_id,
    agent_role,
    SUM(CASE WHEN metric_type = 'Tokens_In' THEN metric_value ELSE 0 END) as input_tokens,
    SUM(CASE WHEN metric_type = 'Tokens_Out' THEN metric_value ELSE 0 END) as output_tokens,
    SUM(CASE WHEN metric_type = 'API_Cost' THEN metric_value ELSE 0 END) as cost
FROM agent_cost_vector
WHERE task_id = 'TASK-2025-001'
GROUP BY task_id, agent_role;
```

#### 3. Slowest Agent Executions

```sql
SELECT
    task_id,
    agent_role,
    metric_value as latency_ms,
    timestamp,
    llm_model
FROM agent_cost_vector
WHERE metric_type = 'Latency'
ORDER BY metric_value DESC
LIMIT 10;
```

#### 4. Cost Breakdown by LLM Model

```sql
SELECT
    llm_model,
    COUNT(*) as call_count,
    SUM(CASE WHEN metric_type = 'API_Cost' THEN metric_value ELSE 0 END) as total_cost,
    AVG(CASE WHEN metric_type = 'API_Cost' THEN metric_value ELSE 0 END) as avg_cost_per_call
FROM agent_cost_vector
WHERE llm_model IS NOT NULL
GROUP BY llm_model
ORDER BY total_cost DESC;
```

#### 5. Daily Cost Trend

```sql
SELECT
    DATE(timestamp) as date,
    agent_role,
    SUM(CASE WHEN metric_type = 'API_Cost' THEN metric_value ELSE 0 END) as daily_cost
FROM agent_cost_vector
GROUP BY DATE(timestamp), agent_role
ORDER BY date DESC, daily_cost DESC;
```

### Defect Analysis

#### 6. Defects by Severity

```sql
SELECT
    severity,
    COUNT(*) as defect_count,
    COUNT(CASE WHEN flagged_by_agent = 1 THEN 1 END) as agent_flagged,
    COUNT(CASE WHEN flagged_by_agent = 0 THEN 1 END) as human_flagged
FROM defect_log
GROUP BY severity
ORDER BY
    CASE severity
        WHEN 'Critical' THEN 1
        WHEN 'High' THEN 2
        WHEN 'Medium' THEN 3
        WHEN 'Low' THEN 4
    END;
```

#### 7. Defects by Type

```sql
SELECT
    defect_type,
    COUNT(*) as count,
    AVG(JULIANDAY(phase_removed) - JULIANDAY(phase_injected)) as avg_phases_to_detect
FROM defect_log
GROUP BY defect_type
ORDER BY count DESC;
```

#### 8. Phase Transition Analysis

```sql
SELECT
    phase_injected,
    phase_removed,
    COUNT(*) as defect_count,
    AVG(CAST((JULIANDAY('now') - JULIANDAY(created_at)) AS REAL)) as avg_age_days
FROM defect_log
GROUP BY phase_injected, phase_removed
ORDER BY defect_count DESC;
```

#### 9. Recent Critical Defects

```sql
SELECT
    defect_id,
    task_id,
    defect_type,
    description,
    component_path,
    created_at
FROM defect_log
WHERE severity = 'Critical'
ORDER BY created_at DESC
LIMIT 10;
```

### Bootstrap Dataset Analysis

#### 10. Bootstrap Collection Progress

```sql
SELECT
    agent_role,
    COUNT(DISTINCT task_id) as unique_tasks,
    COUNT(*) as total_metrics,
    SUM(CASE WHEN metric_type = 'API_Cost' THEN metric_value ELSE 0 END) as total_cost
FROM agent_cost_vector
GROUP BY agent_role
ORDER BY unique_tasks DESC;
```

#### 11. Task Complexity Distribution

```sql
SELECT
    task_id,
    agent_role,
    SUM(CASE WHEN metric_type = 'Latency' THEN metric_value ELSE 0 END) as total_latency,
    SUM(CASE WHEN metric_type = 'Tokens_In' THEN metric_value ELSE 0 END) +
    SUM(CASE WHEN metric_type = 'Tokens_Out' THEN metric_value ELSE 0 END) as total_tokens
FROM agent_cost_vector
GROUP BY task_id, agent_role
ORDER BY total_tokens DESC;
```

---

## Python Helper Scripts

### Script 1: View All Metrics for a Task

```python
#!/usr/bin/env python3
"""View all metrics for a specific task."""
import sqlite3
import sys

def view_task_metrics(task_id: str):
    conn = sqlite3.connect('data/asp_telemetry.db')
    conn.row_factory = sqlite3.Row
    cursor = conn.cursor()

    cursor.execute("""
        SELECT
            timestamp,
            agent_role,
            metric_type,
            metric_value,
            metric_unit,
            llm_model
        FROM agent_cost_vector
        WHERE task_id = ?
        ORDER BY timestamp
    """, (task_id,))

    print(f"\nMetrics for Task: {task_id}")
    print("-" * 80)

    for row in cursor.fetchall():
        print(f"{row['timestamp'][:19]} | {row['agent_role']:12} | "
              f"{row['metric_type']:12} | {row['metric_value']:8.2f} {row['metric_unit']}")

    conn.close()

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Usage: python view_task_metrics.py TASK-ID")
        sys.exit(1)

    view_task_metrics(sys.argv[1])
```

### Script 2: Cost Report Generator

```python
#!/usr/bin/env python3
"""Generate cost report for all agents."""
import sqlite3
from datetime import datetime, timedelta

def generate_cost_report(days: int = 7):
    conn = sqlite3.connect('data/asp_telemetry.db')
    cursor = conn.cursor()

    cutoff_date = (datetime.now() - timedelta(days=days)).isoformat()

    cursor.execute("""
        SELECT
            agent_role,
            COUNT(DISTINCT task_id) as unique_tasks,
            SUM(CASE WHEN metric_type = 'API_Cost' THEN metric_value ELSE 0 END) as total_cost,
            SUM(CASE WHEN metric_type = 'Tokens_In' THEN metric_value ELSE 0 END) as input_tokens,
            SUM(CASE WHEN metric_type = 'Tokens_Out' THEN metric_value ELSE 0 END) as output_tokens,
            AVG(CASE WHEN metric_type = 'Latency' THEN metric_value ELSE 0 END) as avg_latency
        FROM agent_cost_vector
        WHERE timestamp >= ?
        GROUP BY agent_role
        ORDER BY total_cost DESC
    """, (cutoff_date,))

    print(f"\nCost Report - Last {days} Days")
    print("=" * 100)
    print(f"{'Agent':<15} {'Tasks':<8} {'Cost (USD)':<12} {'Input Tokens':<15} {'Output Tokens':<15} {'Avg Latency':<12}")
    print("-" * 100)

    total_cost = 0
    for row in cursor.fetchall():
        agent, tasks, cost, in_tokens, out_tokens, latency = row
        total_cost += cost
        print(f"{agent:<15} {tasks:<8} ${cost:<11.4f} {in_tokens:<15.0f} {out_tokens:<15.0f} {latency:<11.0f}ms")

    print("-" * 100)
    print(f"{'TOTAL':<24} ${total_cost:.4f}")
    print("=" * 100)

    conn.close()

if __name__ == "__main__":
    generate_cost_report(days=30)
```

### Script 3: Defect Summary

```python
#!/usr/bin/env python3
"""Generate defect summary report."""
import sqlite3

def defect_summary():
    conn = sqlite3.connect('data/asp_telemetry.db')
    cursor = conn.cursor()

    print("\nDefect Summary Report")
    print("=" * 80)

    # By severity
    cursor.execute("""
        SELECT severity, COUNT(*) as count
        FROM defect_log
        GROUP BY severity
        ORDER BY
            CASE severity
                WHEN 'Critical' THEN 1
                WHEN 'High' THEN 2
                WHEN 'Medium' THEN 3
                WHEN 'Low' THEN 4
            END
    """)

    print("\nBy Severity:")
    print("-" * 40)
    for severity, count in cursor.fetchall():
        print(f"{severity:<12}: {count:>3} defects")

    # By type
    cursor.execute("""
        SELECT defect_type, COUNT(*) as count
        FROM defect_log
        GROUP BY defect_type
        ORDER BY count DESC
        LIMIT 5
    """)

    print("\nTop 5 Defect Types:")
    print("-" * 40)
    for defect_type, count in cursor.fetchall():
        print(f"{defect_type:<40}: {count:>3}")

    # Agent detection rate
    cursor.execute("""
        SELECT
            COUNT(CASE WHEN flagged_by_agent = 1 THEN 1 END) as agent_detected,
            COUNT(CASE WHEN flagged_by_agent = 0 THEN 1 END) as human_detected,
            COUNT(*) as total
        FROM defect_log
    """)

    agent, human, total = cursor.fetchone()
    print("\nDetection Source:")
    print("-" * 40)
    print(f"Agent-detected : {agent:>3} ({100*agent/total if total > 0 else 0:.1f}%)")
    print(f"Human-detected : {human:>3} ({100*human/total if total > 0 else 0:.1f}%)")
    print(f"Total          : {total:>3}")

    conn.close()

if __name__ == "__main__":
    defect_summary()
```

---

## Troubleshooting

### Issue: No Data in Langfuse Dashboard

**Possible causes:**
1. API keys not configured correctly
2. Telemetry not enabled
3. Agent code not using telemetry decorators

**Solutions:**
1. Verify environment variables:
   ```bash
   echo $LANGFUSE_PUBLIC_KEY
   echo $LANGFUSE_SECRET_KEY
   echo $LANGFUSE_HOST
   ```

2. Check if telemetry is enabled in code:
   ```python
   from asp.telemetry import get_langfuse_client
   client = get_langfuse_client()
   print(client)  # Should not error
   ```

3. Ensure agents use `@track_agent_cost` decorator

### Issue: Database Connection Errors

**Error:** `sqlite3.OperationalError: unable to open database file`

**Solution:**
1. Check database exists:
   ```bash
   ls -la data/asp_telemetry.db
   ```

2. Initialize database if missing:
   ```bash
   uv run python scripts/init_database.py
   ```

3. Check file permissions:
   ```bash
   chmod 644 data/asp_telemetry.db
   ```

### Issue: Missing Metrics

**Problem:** Some metrics not appearing in queries

**Solutions:**
1. Check if decorator is properly applied:
   ```python
   @track_agent_cost(agent_role="Planning", llm_model="claude-sonnet-4")
   def my_agent_function(task_id: str):
       # ...
   ```

2. Verify task_id parameter name matches:
   ```python
   # If parameter is named differently
   @track_agent_cost(agent_role="Planning", task_id_param="custom_task_id")
   def my_agent_function(custom_task_id: str):
       # ...
   ```

3. Check for telemetry errors in logs:
   ```bash
   # Look for "Warning: Failed to log telemetry"
   uv run python your_script.py 2>&1 | grep -i telemetry
   ```

### Issue: High API Costs

**Problem:** Unexpected high costs in Langfuse

**Solutions:**
1. Query expensive operations:
   ```sql
   SELECT task_id, agent_role, metric_value as cost
   FROM agent_cost_vector
   WHERE metric_type = 'API_Cost'
   ORDER BY metric_value DESC
   LIMIT 10;
   ```

2. Check token usage patterns:
   ```sql
   SELECT
       llm_model,
       AVG(CASE WHEN metric_type = 'Tokens_In' THEN metric_value END) as avg_input,
       AVG(CASE WHEN metric_type = 'Tokens_Out' THEN metric_value END) as avg_output
   FROM agent_cost_vector
   GROUP BY llm_model;
   ```

3. Consider using caching or cheaper models for simple tasks

### Issue: Langfuse Flush Errors

**Error:** `Warning: Failed to log to Langfuse`

**Solutions:**
1. Network connectivity issue - check internet connection
2. Rate limiting - add delays between calls
3. Invalid API keys - verify credentials
4. Langfuse service outage - check status at https://status.langfuse.com

---

## Best Practices

### 1. Regular Monitoring

- Check Langfuse dashboard daily during active development
- Review cost trends weekly
- Analyze defect patterns monthly

### 2. Data Retention

- Local database persists indefinitely
- Back up `data/asp_telemetry.db` regularly
- Langfuse free tier has data retention limits (check your plan)

### 3. Query Optimization

- Use indexes effectively (task_id, agent_role, timestamp)
- Add `LIMIT` clauses to large queries
- Create views for frequently-used queries

### 4. Cost Management

- Set up alerts in Langfuse for cost thresholds
- Monitor token usage per agent
- Optimize prompts to reduce token consumption

### 5. Defect Tracking

- Always include detailed descriptions
- Fill in component_path, function_name, line_number when possible
- Add root_cause analysis for critical defects
- Track resolution patterns to prevent recurrence

---

## Additional Resources

### Documentation
- `examples/telemetry_example.py` - Example usage patterns
- `src/asp/telemetry/telemetry.py` - Source code documentation
- `database/README.md` - Database schema details

### External Links
- [Langfuse Documentation](https://langfuse.com/docs)
- [SQLite Documentation](https://www.sqlite.org/docs.html)
- [PROBE-AI Effort Estimation Model](../PRD.md#phase-2-probe-ai-integration)

### Support
- GitHub Issues: Report bugs and feature requests
- Project README: General usage information

---

## Quick Reference

### Langfuse URL
```
https://us.cloud.langfuse.com
```

### Database Path
```
data/asp_telemetry.db
```

### Environment Variables
```bash
LANGFUSE_PUBLIC_KEY=pk-lf-...
LANGFUSE_SECRET_KEY=sk-lf-...
LANGFUSE_HOST=https://cloud.langfuse.com
```

### Common Commands
```bash
# View database schema
sqlite3 data/asp_telemetry.db ".schema"

# Export data to CSV
sqlite3 -header -csv data/asp_telemetry.db "SELECT * FROM agent_cost_vector;" > metrics.csv

# Count total records
sqlite3 data/asp_telemetry.db "SELECT COUNT(*) FROM agent_cost_vector;"

# Check last 10 entries
sqlite3 data/asp_telemetry.db "SELECT * FROM agent_cost_vector ORDER BY id DESC LIMIT 10;"
```

---

**Last Updated:** November 17, 2025
**Version:** 1.0
**Maintainer:** ASP Development Team
