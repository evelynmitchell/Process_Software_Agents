{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASP External Repo Workflow Test: nanoGPT\n",
    "\n",
    "This notebook demonstrates the Agentic Software Process (ASP) workflow for working with an external repository inside a Colab environment.\n",
    "\n",
    "**Workflow:**\n",
    "1. Setup environment\n",
    "2. Clone external repository (`nanoGPT`)\n",
    "3. Inject test suite (simulating an agent adding tests)\n",
    "4. Run tests to verify coverage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch numpy transformers tiktoken tqdm pytest pytest-cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/karpathy/nanoGPT.git\n",
    "%cd nanoGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inject Test Suite\n",
    "\n",
    "We will now inject the test files that were generated by the ASP Coding Agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tests/conftest.py\n\"\"\"Pytest configuration and fixtures for nanoGPT tests.\"\"\"\n\nimport sys\nfrom pathlib import Path\n\nimport pytest\n\n# Add parent directory to path for imports\nsys.path.insert(0, str(Path(__file__).parent.parent))\n\n\n@pytest.fixture\ndef small_config():\n    \"\"\"Provide small GPTConfig for fast testing.\n\n    This config creates a tiny model (~50K params) that runs quickly on CPU.\n    \"\"\"\n    from model import GPTConfig\n    return GPTConfig(\n        block_size=32,\n        vocab_size=100,\n        n_layer=2,\n        n_head=4,\n        n_embd=64,\n        dropout=0.0,\n        bias=True\n    )\n\n\n@pytest.fixture\ndef tiny_config():\n    \"\"\"Provide tiny GPTConfig for integration tests.\n\n    Even smaller than small_config for tests that need to train.\n    \"\"\"\n    from model import GPTConfig\n    return GPTConfig(\n        block_size=16,\n        vocab_size=50,\n        n_layer=1,\n        n_head=2,\n        n_embd=32,\n        dropout=0.0,\n        bias=True\n    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tests/test_model.py\n\"\"\"Tests for nanoGPT model.py\n\nThis test suite provides coverage for the core GPT model components:\n- GPTConfig dataclass\n- LayerNorm (custom implementation)\n- CausalSelfAttention\n- MLP (feed-forward network)\n- Block (transformer block)\n- GPT (full model)\n\nRun with: pytest tests/test_model.py -v\nCoverage: pytest tests/test_model.py --cov=model --cov-report=term-missing\n\"\"\"\n\nimport pytest\nimport torch\n\nfrom model import GPTConfig, LayerNorm, CausalSelfAttention, MLP, Block, GPT\n\n\nclass TestGPTConfig:\n    \"\"\"Tests for GPTConfig dataclass.\"\"\"\n\n    def test_default_config(self):\n        \"\"\"Test default configuration values.\"\"\"\n        config = GPTConfig()\n        assert config.block_size == 1024\n        assert config.vocab_size == 50304\n        assert config.n_layer == 12\n        assert config.n_head == 12\n        assert config.n_embd == 768\n        assert config.dropout == 0.0\n        assert config.bias is True\n\n    def test_custom_config(self):\n        \"\"\"Test custom configuration values.\"\"\"\n        config = GPTConfig(\n            block_size=256,\n            vocab_size=1000,\n            n_layer=4,\n            n_head=4,\n            n_embd=128,\n            dropout=0.1,\n            bias=False\n        )\n        assert config.block_size == 256\n        assert config.vocab_size == 1000\n        assert config.n_layer == 4\n        assert config.n_head == 4\n        assert config.n_embd == 128\n        assert config.dropout == 0.1\n        assert config.bias is False\n\n    def test_small_config_for_testing(self):\n        \"\"\"Test minimal config for fast testing.\"\"\"\n        config = GPTConfig(\n            block_size=32,\n            vocab_size=100,\n            n_layer=2,\n            n_head=2,\n            n_embd=64,\n            dropout=0.0,\n            bias=True\n        )\n        assert config.n_embd % config.n_head == 0  # Must be divisible\n\n\nclass TestLayerNorm:\n    \"\"\"Tests for custom LayerNorm implementation.\"\"\"\n\n    def test_layer_norm_with_bias(self):\n        \"\"\"Test LayerNorm with bias enabled.\"\"\"\n        ln = LayerNorm(ndim=64, bias=True)\n        assert ln.weight.shape == (64,)\n        assert ln.bias is not None\n        assert ln.bias.shape == (64,)\n\n    def test_layer_norm_without_bias(self):\n        \"\"\"Test LayerNorm without bias.\"\"\"\n        ln = LayerNorm(ndim=64, bias=False)\n        assert ln.weight.shape == (64,)\n        assert ln.bias is None\n\n    def test_layer_norm_forward(self):\n        \"\"\"Test LayerNorm forward pass.\"\"\"\n        ln = LayerNorm(ndim=64, bias=True)\n        x = torch.randn(2, 10, 64)  # batch=2, seq=10, dim=64\n        y = ln(x)\n        assert y.shape == x.shape\n\n    def test_layer_norm_normalization(self):\n        \"\"\"Test that output is approximately normalized.\"\"\"\n        ln = LayerNorm(ndim=64, bias=False)\n        # Reset weights to default for predictable behavior\n        ln.weight.data.fill_(1.0)\n        x = torch.randn(2, 10, 64)\n        y = ln(x)\n        # Check that mean is close to 0 and std close to 1\n        assert y.mean(dim=-1).abs().max() < 0.1\n        assert (y.std(dim=-1) - 1.0).abs().max() < 0.1\n\n\nclass TestCausalSelfAttention:\n    \"\"\"Tests for CausalSelfAttention module.\"\"\"\n\n    def test_attention_init(self, small_config):\n        \"\"\"Test attention initialization.\"\"\"\n        attn = CausalSelfAttention(small_config)\n        assert attn.n_head == 4\n        assert attn.n_embd == 64\n        assert attn.dropout == 0.0\n\n    def test_attention_forward(self, small_config):\n        \"\"\"Test attention forward pass.\"\"\"\n        attn = CausalSelfAttention(small_config)\n        x = torch.randn(2, 16, 64)  # batch=2, seq=16, dim=64\n        y = attn(x)\n        assert y.shape == x.shape\n\n    def test_attention_different_seq_lengths(self, small_config):\n        \"\"\"Test attention with various sequence lengths.\"\"\"\n        attn = CausalSelfAttention(small_config)\n        for seq_len in [1, 8, 16, 32]:\n            x = torch.randn(2, seq_len, 64)\n            y = attn(x)\n            assert y.shape == x.shape\n\n\nclass TestMLP:\n    \"\"\"Tests for MLP (feed-forward) module.\"\"\"\n\n    def test_mlp_init(self, small_config):\n        \"\"\"Test MLP initialization.\"\"\"\n        mlp = MLP(small_config)\n        # First layer expands 4x\n        assert mlp.c_fc.in_features == 64\n        assert mlp.c_fc.out_features == 256\n        # Second layer projects back\n        assert mlp.c_proj.in_features == 256\n        assert mlp.c_proj.out_features == 64\n\n    def test_mlp_forward(self, small_config):\n        \"\"\"Test MLP forward pass.\"\"\"\n        mlp = MLP(small_config)\n        x = torch.randn(2, 16, 64)\n        y = mlp(x)\n        assert y.shape == x.shape\n\n\nclass TestBlock:\n    \"\"\"Tests for transformer Block.\"\"\"\n\n    def test_block_init(self, small_config):\n        \"\"\"Test Block initialization.\"\"\"\n        block = Block(small_config)\n        assert isinstance(block.ln_1, LayerNorm)\n        assert isinstance(block.attn, CausalSelfAttention)\n        assert isinstance(block.ln_2, LayerNorm)\n        assert isinstance(block.mlp, MLP)\n\n    def test_block_forward(self, small_config):\n        \"\"\"Test Block forward pass.\"\"\"\n        block = Block(small_config)\n        x = torch.randn(2, 16, 64)\n        y = block(x)\n        assert y.shape == x.shape\n\n    def test_block_residual_connections(self, small_config):\n        \"\"\"Test that block uses residual connections.\"\"\"\n        block = Block(small_config)\n        x = torch.randn(2, 16, 64)\n        y = block(x)\n        # Output should not be identical to input (transformations applied)\n        assert not torch.allclose(x, y)\n\n\nclass TestGPT:\n    \"\"\"Tests for full GPT model.\"\"\"\n\n    def test_gpt_init(self, small_config, capsys):\n        \"\"\"Test GPT initialization.\"\"\"\n        model = GPT(small_config)\n        assert model.config == small_config\n        assert len(model.transformer.h) == 2  # n_layer\n        # Check parameter count is printed\n        captured = capsys.readouterr()\n        assert \"number of parameters\" in captured.out\n\n    def test_gpt_forward_no_targets(self, small_config):\n        \"\"\"Test GPT forward pass without targets (inference).\"\"\"\n        model = GPT(small_config)\n        model.eval()\n        idx = torch.randint(0, 100, (2, 16))  # batch=2, seq=16\n        logits, loss = model(idx)\n        assert logits.shape == (2, 1, 100)  # Only last position\n        assert loss is None\n\n    def test_gpt_forward_with_targets(self, small_config):\n        \"\"\"Test GPT forward pass with targets (training).\"\"\"\n        model = GPT(small_config)\n        idx = torch.randint(0, 100, (2, 16))\n        targets = torch.randint(0, 100, (2, 16))\n        logits, loss = model(idx, targets)\n        assert logits.shape == (2, 16, 100)  # All positions\n        assert loss is not None\n        assert loss.item() > 0\n\n    def test_gpt_get_num_params(self, small_config):\n        \"\"\"Test parameter counting.\"\"\"\n        model = GPT(small_config)\n        n_params = model.get_num_params()\n        n_params_with_emb = model.get_num_params(non_embedding=False)\n        assert n_params > 0\n        assert n_params_with_emb > n_params  # Includes position embeddings\n\n    def test_gpt_crop_block_size(self, small_config):\n        \"\"\"Test cropping block size.\"\"\"\n        model = GPT(small_config)\n        model.crop_block_size(16)\n        assert model.config.block_size == 16\n        assert model.transformer.wpe.weight.shape[0] == 16\n\n    def test_gpt_generate(self, small_config):\n        \"\"\"Test text generation.\"\"\"\n        model = GPT(small_config)\n        model.eval()\n        idx = torch.randint(0, 100, (1, 5))  # Start with 5 tokens\n        generated = model.generate(idx, max_new_tokens=10)\n        assert generated.shape == (1, 15)  # 5 + 10 new tokens\n\n    def test_gpt_generate_with_temperature(self, small_config):\n        \"\"\"Test generation with temperature scaling.\"\"\"\n        model = GPT(small_config)\n        model.eval()\n        idx = torch.randint(0, 100, (1, 5))\n        # Low temperature should give more deterministic outputs\n        generated = model.generate(idx, max_new_tokens=5, temperature=0.1)\n        assert generated.shape == (1, 10)\n\n    def test_gpt_generate_with_top_k(self, small_config):\n        \"\"\"Test generation with top-k sampling.\"\"\"\n        model = GPT(small_config)\n        model.eval()\n        idx = torch.randint(0, 100, (1, 5))\n        generated = model.generate(idx, max_new_tokens=5, top_k=10)\n        assert generated.shape == (1, 10)\n\n    def test_gpt_configure_optimizers(self, small_config):\n        \"\"\"Test optimizer configuration.\"\"\"\n        model = GPT(small_config)\n        optimizer = model.configure_optimizers(\n            weight_decay=0.1,\n            learning_rate=1e-4,\n            betas=(0.9, 0.95),\n            device_type='cpu'\n        )\n        assert isinstance(optimizer, torch.optim.AdamW)\n        assert len(optimizer.param_groups) == 2  # decay and no-decay groups\n\n    def test_gpt_estimate_mfu(self, small_config):\n        \"\"\"Test MFU estimation.\"\"\"\n        model = GPT(small_config)\n        mfu = model.estimate_mfu(fwdbwd_per_iter=1, dt=1.0)\n        assert mfu > 0\n        assert mfu < 1  # Should be less than 100% utilization\n\n    def test_gpt_sequence_too_long(self, small_config):\n        \"\"\"Test that too-long sequences raise assertion.\"\"\"\n        model = GPT(small_config)\n        idx = torch.randint(0, 100, (1, 64))  # Longer than block_size=32\n        with pytest.raises(AssertionError):\n            model(idx)\n\n\nclass TestGPTIntegration:\n    \"\"\"Integration tests for GPT training loop.\"\"\"\n\n    def test_training_step(self, tiny_config):\n        \"\"\"Test a single training step.\"\"\"\n        model = GPT(tiny_config)\n        optimizer = model.configure_optimizers(\n            weight_decay=0.1,\n            learning_rate=1e-3,\n            betas=(0.9, 0.95),\n            device_type='cpu'\n        )\n\n        # Training data\n        idx = torch.randint(0, 50, (4, 16))\n        targets = torch.randint(0, 50, (4, 16))\n\n        # Forward pass\n        logits, loss = model(idx, targets)\n\n        # Backward pass\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n        assert loss.item() > 0\n\n    def test_overfitting_small_batch(self, tiny_config):\n        \"\"\"Test that model can overfit a small batch.\"\"\"\n        model = GPT(tiny_config)\n        optimizer = model.configure_optimizers(\n            weight_decay=0.0,\n            learning_rate=1e-2,\n            betas=(0.9, 0.95),\n            device_type='cpu'\n        )\n\n        # Fixed small batch to overfit\n        torch.manual_seed(42)\n        idx = torch.randint(0, 50, (2, 8))\n        targets = torch.randint(0, 50, (2, 8))\n\n        initial_loss = None\n        for _ in range(50):\n            logits, loss = model(idx, targets)\n            if initial_loss is None:\n                initial_loss = loss.item()\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n        final_loss = loss.item()\n        # Loss should decrease significantly\n        assert final_loss < initial_loss * 0.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Tests\n",
    "\n",
    "Now we run the tests to verify the `model.py` implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest tests/test_model.py -v --cov=model --cov-report=term-missing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}